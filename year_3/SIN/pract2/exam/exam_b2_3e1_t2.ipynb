{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems SIN 2025/26, lab group 3E1, subgroup **2**.\n",
    "# **A3**. Block 2 lab exam (1.25 points).\n",
    "## 17 December 2025. **TIME: 45 min**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student: Sergio Quintanilla GraciÃ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/sergio/Documents/GitHub/university/year_3/SIN/pract2/.venv/lib64/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/sergio/Documents/GitHub/university/year_3/SIN/pract2/.venv/lib64/python3.13/site-packages (from scikit-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/sergio/Documents/GitHub/university/year_3/SIN/pract2/.venv/lib64/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sergio/Documents/GitHub/university/year_3/SIN/pract2/.venv/lib64/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sergio/Documents/GitHub/university/year_3/SIN/pract2/.venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting this exam, you must download the dataset (samples **X.npy** and labels **y.npy**) and the logistic regression library (**Logistic_Regression.py**) from Poliformat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to execute this cell if running in Google Colab \n",
    "\n",
    "# You need to upload LogisticRegression.py\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# You need to upload X.npy \n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# You need to upload y.npy\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset description**: This dataset contains **1500** samples with **254** features, for a classification task into **29** classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code below, you will load the dataset and partition it into 80% of the samples for training and 20% for testing (with seed random_state$=$23), and you will obtain a baseline test error rate (of 12.3%) from a logistic regression classifier trained using default parameters and batch size 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 12.3%\n"
     ]
    }
   ],
   "source": [
    "from LogisticRegression import LogisticRegressionClassification, LogisticRegressionTraining\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "X = np.load('X.npy'); y = np.load('y.npy')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)\n",
    "\n",
    "# Train and classify\n",
    "W = LogisticRegressionTraining(X_train, y_train, bs=10)\n",
    "haty_test = LogisticRegressionClassification(X_test, W)\n",
    "accuracy = np.sum(haty_test==y_test)/M\n",
    "print(f\"Test error: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Applying the logistic regression classifier with default parameter values (eta 1e-2, batch size 10), explore the maximum number of epochs in logarithmic scale to determine an optimal value.\n",
    "\n",
    "According to the results:\n",
    "\n",
    "1.1) What is the optimal value for the maximum numer of epochs? How do you decide which one to choose?\n",
    "\n",
    "1.2) Is overfitting/overtraining observed? If so, from what epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-02     5.2%   12.3%\n",
      "  10     10 1.0e-02     4.0%   12.3%\n",
      "  10     20 1.0e-02     2.0%    9.3%\n",
      "  10     50 1.0e-02     0.7%   11.3%\n",
      "  10    100 1.0e-02     0.2%   12.3%\n",
      "  10    200 1.0e-02     0.0%   12.3%\n",
      "  10    500 1.0e-02     0.0%   12.0%\n",
      "  10   1000 1.0e-02     0.0%   12.0%\n",
      "  10   2000 1.0e-02     0.0%   12.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "# Write here the code for this task\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**<br><br>\n",
    "1.1) The optimal value for the maximum number of epochs is 20, as with that number of epochs we get the lowest test error (9.3%). With more than 20 epochs, even though the train error decreases, the test error increases. This is why we choose this number of epochs.\n",
    "\n",
    "1.2) Overfitting is observed from 50 epochs onwards. This is because the train error decreases, but the test error starts increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Using maximum number of epochs 1000, batch size (bs) 10 and applying the logistic regression classifier, adjust the learning rate (eta) in logarithmic scale to determine an optimal value. Report the classification error rate on the training and test sets.\n",
    "\n",
    "Based on the results, what is the optimal value for the learning rate? Discuss the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10   1000 1.0e-04     3.1%    9.3%\n",
      "  10   1000 1.0e-03     0.2%   12.0%\n",
      "  10   1000 1.0e-02     0.0%   12.0%\n",
      "  10   1000 1.0e-01     2.7%   14.7%\n",
      "  10   1000 1.0e+00     9.8%   22.0%\n",
      "  10   1000 1.0e+01    10.4%   15.0%\n",
      "  10   1000 1.0e+02     8.3%   16.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "# Write here the code for this task\n",
    "bs = 10\n",
    "maxEpochs = 1000\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "As we can see in the results, the lowest test error is seen with the lowest ETA (1e-4), and it then starts increasing up to a point (ETA = 1). After that, it slightly decreases.\n",
    "Regarding the train error, in general, it seems to increase as the ETA increases, even though with lower ETAs it doesn't reach 0%.\n",
    "\n",
    "The optimal value for the learning rate (ETA) would be 1e-4, as with this value, the lowest test error is achieved, even though the train error doesn't reach 0% as it does with ETA = 1e-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Applying the logistic regression classifier with default parameter values and\n",
    "learning rate (eta) 1e-2, explore at the same time both the maximum number of epochs and the batch size\n",
    "(bs) to determine their optimal values together (tip: use a nested loop, with bs in the outer loop for easier interpretability). Use in both cases a logarithmic\n",
    "scale. Report the classification error rate on the training and test\n",
    "sets.\n",
    "\n",
    "Based on the results, what are the optimal values? Discuss the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "   1      5 1.0e-02    21.8%   25.7%\n",
      "   1     10 1.0e-02    21.8%   25.7%\n",
      "   1     20 1.0e-02    21.8%   25.7%\n",
      "   1     50 1.0e-02    21.8%   25.7%\n",
      "   1    100 1.0e-02    21.8%   25.7%\n",
      "   1    200 1.0e-02    21.8%   25.7%\n",
      "   1    500 1.0e-02    21.8%   25.7%\n",
      "   1   1000 1.0e-02    21.8%   25.7%\n",
      "   1   2000 1.0e-02    21.8%   25.7%\n",
      "   2      5 1.0e-02    10.7%   20.7%\n",
      "   2     10 1.0e-02     7.4%   16.0%\n",
      "   2     20 1.0e-02     1.5%   13.3%\n",
      "   2     50 1.0e-02     1.5%   13.3%\n",
      "   2    100 1.0e-02     1.5%   13.3%\n",
      "   2    200 1.0e-02     1.5%   13.3%\n",
      "   2    500 1.0e-02     1.5%   13.3%\n",
      "   2   1000 1.0e-02     1.5%   13.3%\n",
      "   2   2000 1.0e-02     1.5%   13.3%\n",
      "   5      5 1.0e-02     5.8%   14.7%\n",
      "   5     10 1.0e-02     3.9%   12.0%\n",
      "   5     20 1.0e-02     1.3%   10.3%\n",
      "   5     50 1.0e-02     0.2%   12.7%\n",
      "   5    100 1.0e-02     0.0%   12.7%\n",
      "   5    200 1.0e-02     0.0%   12.0%\n",
      "   5    500 1.0e-02     0.0%   12.0%\n",
      "   5   1000 1.0e-02     0.0%   12.0%\n",
      "   5   2000 1.0e-02     0.0%   12.0%\n",
      "  10      5 1.0e-02     5.2%   12.3%\n",
      "  10     10 1.0e-02     4.0%   12.3%\n",
      "  10     20 1.0e-02     2.0%    9.3%\n",
      "  10     50 1.0e-02     0.7%   11.3%\n",
      "  10    100 1.0e-02     0.2%   12.3%\n",
      "  10    200 1.0e-02     0.0%   12.3%\n",
      "  10    500 1.0e-02     0.0%   12.0%\n",
      "  10   1000 1.0e-02     0.0%   12.0%\n",
      "  10   2000 1.0e-02     0.0%   12.0%\n",
      "  20      5 1.0e-02     5.8%   10.3%\n",
      "  20     10 1.0e-02     5.0%    9.7%\n",
      "  20     20 1.0e-02     2.9%    9.3%\n",
      "  20     50 1.0e-02     1.3%   10.0%\n",
      "  20    100 1.0e-02     0.7%   11.3%\n",
      "  20    200 1.0e-02     0.2%   12.3%\n",
      "  20    500 1.0e-02     0.0%   12.3%\n",
      "  20   1000 1.0e-02     0.0%   12.0%\n",
      "  20   2000 1.0e-02     0.0%   12.0%\n",
      "  50      5 1.0e-02     6.4%   10.0%\n",
      "  50     10 1.0e-02     6.1%   10.3%\n",
      "  50     20 1.0e-02     4.5%   10.0%\n",
      "  50     50 1.0e-02     3.1%    9.7%\n",
      "  50    100 1.0e-02     1.8%   10.3%\n",
      "  50    200 1.0e-02     0.9%   11.0%\n",
      "  50    500 1.0e-02     0.2%   12.0%\n",
      "  50   1000 1.0e-02     0.0%   12.0%\n",
      "  50   2000 1.0e-02     0.0%   12.0%\n",
      " 100      5 1.0e-02     7.3%    9.7%\n",
      " 100     10 1.0e-02     6.7%    9.7%\n",
      " 100     20 1.0e-02     5.8%    9.7%\n",
      " 100     50 1.0e-02     3.8%    9.7%\n",
      " 100    100 1.0e-02     3.1%    9.7%\n",
      " 100    200 1.0e-02     1.8%   10.0%\n",
      " 100    500 1.0e-02     0.7%   11.0%\n",
      " 100   1000 1.0e-02     0.2%   12.0%\n",
      " 100   2000 1.0e-02     0.0%   12.3%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "# Write here the code for this task\n",
    "eta = 1e-2\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "        W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "        haty_train = LogisticRegressionClassification(X_train,W)\n",
    "        acc_train = np.sum(haty_train==y_train)/N\n",
    "        haty_test = LogisticRegressionClassification(X_test,W)\n",
    "        acc_test = np.sum(haty_test==y_test)/M\n",
    "        print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "As we can observe in the results, the optimal values would be a batch size of 10, and a max number of epochs of 20. This combination shows the lowest test error (9.3%) with a low train error (2.0%) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "According to the results you have obtained, can you claim whether this task is linearly separable? Which result(s) can you look at for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Based on the results obtained on exercise 3, we can say that the task is linearly separable. this is because we can observe that, in several instances, the train error reaches 0 (for example, for batch size = 50, max epochs = 1000, and learning rate = 1e-2). Even though these combinations don't use the optimal values, they show that it's possible to linearly separate the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
