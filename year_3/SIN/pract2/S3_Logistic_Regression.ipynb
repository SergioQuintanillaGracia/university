{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3\n",
    "\n",
    "<div style=\"text-align: justify\">In this third session we will apply Logistic Regression to some classification tasks. A simple implementation of Logistic Regression and its application is provided. The final purpose of this session is to apply Logistic Regression to MyDigits dataset to train a matrix of weights that will be used in an application for Handwritten Digit Classification.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to run this code if this is the first time you are running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in ./.venv/lib64/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib64/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib64/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: pillow in ./.venv/lib64/python3.13/site-packages (11.3.0)\n",
      "Requirement already satisfied: gradio in ./.venv/lib64/python3.13/site-packages (5.49.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib64/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib64/python3.13/site-packages (from seaborn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib64/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.121.2)\n",
      "Requirement already satisfied: ffmpy in ./.venv/lib64/python3.13/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==1.13.3 in ./.venv/lib64/python3.13/site-packages (from gradio) (1.13.3)\n",
      "Requirement already satisfied: groovy~=0.1 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in ./.venv/lib64/python3.13/site-packages (from gradio) (1.1.4)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: orjson~=3.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: packaging in ./.venv/lib64/python3.13/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.11.10)\n",
      "Requirement already satisfied: pydub in ./.venv/lib64/python3.13/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: ruff>=0.9.3 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.14.5)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.49.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib64/python3.13/site-packages (from gradio-client==1.13.3->gradio) (2025.10.0)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in ./.venv/lib64/python3.13/site-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in ./.venv/lib64/python3.13/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib64/python3.13/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib64/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn scikit-learn pandas pillow gradio matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoding:** $\\;$ function to convert class labels into one-hot encoding representation\n",
    "\n",
    "$$\\operatorname{one-hot}(y)%\n",
    "=\\boldsymbol{y}%\n",
    "=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_C\\end{pmatrix}%\n",
    "=\\begin{pmatrix}\\mathbb{I}(y=1)\\\\\\vdots\\\\\\mathbb{I}(y=C)\\end{pmatrix}\\in\\{0,1\\}^C%\n",
    "\\quad\\text{with}\\quad%\n",
    "\\sum_c y_c=1$$\n",
    "\n",
    "$\\;$ where $y$ is a categorical variable that takes a value among $C$ possible categories, $\\,\\{1,\\dotsc,C\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoding(y):\n",
    "  # Unique class labels  \n",
    "  c = np.unique(y); C = c.size;\n",
    "\n",
    "  # Mapping class labels from 0 to C-1\n",
    "  # Create a mapping from values in c to their corresponding indices\n",
    "  mapping = {value: idx for idx, value in enumerate(c)}\n",
    "  # Convert y using the mapping\n",
    "  y = np.vectorize(mapping.get)(y)\n",
    "\n",
    "  # Generate one-hot encoding using np.eye\n",
    "  return np.eye(C)[y] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ComputeGradient:** $\\;$ weight update for logistic regression\n",
    "\n",
    "Gradient descent applied to logistic regression:\n",
    "\n",
    "Neg-log-likelihood (NLL): $\\;$ the NLL is a convex objective function\n",
    "$$\\operatorname{NLL}(\\mathbf{W})%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n)\n",
    "\\qquad\\text{with}\\quad%\n",
    "\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)%\n",
    "\\quad\\text{and}\\quad%\n",
    "\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$$\n",
    "\n",
    "NLL gradient:\n",
    "$$\\begin{pmatrix}%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{11}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{1C}}\\\\%\n",
    "\\vdots&\\ddots&\\vdots\\\\%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{D1}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{DC}}\\\\%\n",
    "\\end{pmatrix}\n",
    "=\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\frac{\\partial(-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n))}{\\partial \\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradient(X, Y, W):\n",
    "  N, D = X.shape\n",
    "  X = np.concatenate((np.ones((N,1)), X),axis=1)\n",
    "  N, C = Y.shape\n",
    "  Z = np.zeros((N, C)).astype(float)\n",
    "  \n",
    "  # Compute logits\n",
    "  a = X @ W\n",
    "  # Normalizing logits for robust computation to avoid overflow\n",
    "  a -= a.max(axis=1, keepdims=True) \n",
    "  # Compute softmax (probabilistic class label)\n",
    "  mu = np.exp(a)\n",
    "  mu /= np.sum(mu, axis=1, keepdims=True)\n",
    "  # Substract actual label from probabilistic class label\n",
    "  Z = mu - Y\n",
    "  # Gradient is the sum over all samples of the outer dot product\n",
    "  return (X.T @ Z)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegressionClassification:** $\\;$ classification of samples provided a weight matrix\n",
    "\n",
    "$$c(\\boldsymbol{x})%\n",
    "=\\operatorname*{argmax}\\limits_c\\;\\boldsymbol{\\mu}_c%\n",
    "\\quad\\text{with}\\quad%\n",
    "\\boldsymbol{\\mu}_c=\\mathcal{S}(\\boldsymbol{a})_c,\\quad%\n",
    "\\boldsymbol{a}=f(\\boldsymbol{x};\\mathbf{W})=\\mathbf{W}^t\\boldsymbol{x},\\quad%\n",
    "\\mathbf{W}\\in\\mathbb{R}^{D\\times C}\\quad\\text{and}\\quad\\boldsymbol{x}\\in\\mathbb{R}^D$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionClassification(X, W):\n",
    "  N, D = X.shape\n",
    "  X = np.concatenate((np.ones((N,1)), X),axis=1)\n",
    "  # Compute logits\n",
    "  a = X @ W\n",
    "  # Normalizing logits for robust computation to avoid overflow\n",
    "  a -= a.max(axis=1, keepdims=True) \n",
    "  # Compute softmax (probabilistic class label)\n",
    "  mu = np.exp(a)\n",
    "  mu /= np.sum(mu, axis=1, keepdims=True)\n",
    "  return np.argmax(mu,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegressionTraining:** $\\;$ implementation of batch training based on stochastic gradient descent\n",
    "\n",
    "Gradient descent applied to logistic regression: $\\displaystyle\\quad\\mathbf{W}_0=\\mathbf{0};\\quad\\mathbf{W}_{i+1}=\\mathbf{W}_i -\\eta_i\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}\\biggr\\vert_{\\mathbf{W}_i}\\quad i=0,1,\\ldots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionTraining(X, y, W=None, bs=1, maxEpochs=10, eta=1e-2, tol=1e-3):\n",
    "  rng = np.random.default_rng(seed=23)\n",
    "  N, D = X.shape;\n",
    "  Y = OneHotEncoding(y)\n",
    "  N, C = Y.shape\n",
    "  if W is None:  # W_0 = 0\n",
    "    W = np.zeros((1+D, C))\n",
    "  grad = np.inf; epoch = 0\n",
    "  while np.max(np.abs(grad)) > tol and epoch < maxEpochs:  # W_i+1 = W_i - eta_i * grad_W_i\n",
    "    perm = rng.permutation(N); Xperm = X[perm]; Yperm = Y[perm]\n",
    "    j = 0\n",
    "    for j in range(N//bs):\n",
    "      X_batch = Xperm[j*bs:(j+1)*bs]\n",
    "      Y_batch = Yperm[j*bs:(j+1)*bs]\n",
    "      grad = ComputeGradient(X_batch, Y_batch, W)\n",
    "      W = W - eta*grad\n",
    "    # If there are remaining samples after splitting into batches\n",
    "    if((j+1)*bs < N):\n",
    "      X_batch = Xperm[(j+1)*bs:]\n",
    "      Y_batch = Yperm[(j+1)*bs:]\n",
    "      grad = ComputeGradient(X_batch, Y_batch, W)\n",
    "      W = W - eta*grad \n",
    "    epoch += 1\n",
    "  return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and partitioning the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris(); X = iris.data.astype(np.float16); y = iris.target.astype(np.uint)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression:** $\\;$ training, classification and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 26.7%\n"
     ]
    }
   ],
   "source": [
    "W = LogisticRegressionTraining(X_train,y_train, bs=10)\n",
    "haty_test = LogisticRegressionClassification(X_test,W)\n",
    "accuracy = np.sum(haty_test==y_test)/M\n",
    "print(f\"Test error: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of epochs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-02    31.7%   23.3%\n",
      "  10     10 1.0e-02    35.0%   26.7%\n",
      "  10     20 1.0e-02    17.5%   10.0%\n",
      "  10     50 1.0e-02     4.2%    6.7%\n",
      "  10    100 1.0e-02     4.2%    0.0%\n",
      "  10    200 1.0e-02     3.3%    3.3%\n",
      "  10    500 1.0e-02     2.5%    3.3%\n",
      "  10   1000 1.0e-02     2.5%    3.3%\n",
      "  10   2000 1.0e-02     1.7%    3.3%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed the effect of *over-training* on the error rates, when the error rate in the training set still decreases while the error rate in the test set starts increasing. We must prevent over-training by applying *early stopping*, setting the maximum number of epochs to, for example, 100 epochs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate ($\\eta$/eta):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10    100 1.0e-04    66.7%   66.7%\n",
      "  10    100 1.0e-03    28.3%   23.3%\n",
      "  10    100 1.0e-02     4.2%    0.0%\n",
      "  10    100 1.0e-01     3.3%    0.0%\n",
      "  10    100 1.0e+00     6.7%    6.7%\n",
      "  10    100 1.0e+01     6.7%    6.7%\n",
      "  10    100 1.0e+02    13.3%   20.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; maxEpochs = 100;\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default learning rate ($\\eta=$ 1e-2) provides good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the batch size:** the *bs* parameter sets every how many samples the weight matrix is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "   1    100 1.0e-02     2.5%    3.3%\n",
      "   2    100 1.0e-02     2.5%    3.3%\n",
      "   5    100 1.0e-02     2.5%    0.0%\n",
      "  10    100 1.0e-02     4.2%    0.0%\n",
      "  20    100 1.0e-02     5.8%    0.0%\n",
      "  50    100 1.0e-02    16.7%   10.0%\n",
      " 100    100 1.0e-02    31.7%   23.3%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "maxEpochs=100; eta=1e-2;\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, a small batch size provides good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression applied to the Digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and partitioning the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = load_digits(); X = digits.images.astype(np.float16).reshape(-1, 8*8); X/=np.max(X)\n",
    "y = digits.target.astype(np.uint);\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of epochs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-02    10.6%   13.3%\n",
      "  10     10 1.0e-02     8.1%   10.8%\n",
      "  10     20 1.0e-02     6.5%    9.2%\n",
      "  10     50 1.0e-02     5.0%    6.4%\n",
      "  10    100 1.0e-02     4.1%    5.0%\n",
      "  10    200 1.0e-02     2.8%    4.2%\n",
      "  10    500 1.0e-02     1.7%    3.3%\n",
      "  10   1000 1.0e-02     1.2%    2.2%\n",
      "  10   2000 1.0e-02     1.1%    1.9%\n",
      "  10   5000 1.0e-02     1.1%    1.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting learning rate (eta):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10   2000 1.0e-04     6.6%    8.9%\n",
      "  10   2000 1.0e-03     2.9%    4.2%\n",
      "  10   2000 1.0e-02     1.1%    1.9%\n",
      "  10   2000 1.0e-01     0.6%    2.8%\n",
      "  10   2000 1.0e+00     0.2%    3.3%\n",
      "  10   2000 1.0e+01     2.5%    4.7%\n",
      "  10   2000 1.0e+02     6.1%    7.5%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; maxEpochs = 2000;\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the batch size:** the *bs* parameter sets every how many samples the weight matrix is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "   1   2000 1.0e-02     1.4%    3.3%\n",
      "   2   2000 1.0e-02     1.8%    3.6%\n",
      "   5   2000 1.0e-02     1.3%    2.8%\n",
      "  10   2000 1.0e-02     1.1%    1.9%\n",
      "  20   2000 1.0e-02     1.2%    2.2%\n",
      "  50   2000 1.0e-02     1.8%    3.3%\n",
      " 100   2000 1.0e-02     2.7%    4.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "maxEpochs=2000; eta=1e-2;\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final classifier:** $\\;$ Training final classifier with all data available and best parameters, saving and loading to test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=10; maxEpochs=2000; eta=1e-2\n",
    "W = LogisticRegressionTraining(X, y, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "np.save(\"DigitsWeights.npy\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error of final classifier: 1.9%\n"
     ]
    }
   ],
   "source": [
    "with open('DigitsWeights.npy', 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "    fd.close()\n",
    "haty_test = LogisticRegressionClassification(X_test,W)\n",
    "accuracy = np.sum(haty_test==y_test)/y_test.size\n",
    "print(f\"Test error of final classifier: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Logistic regression applied to MyDigits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and partitioning the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload your images and labels files\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('images.npy', 'rb') as fd:\n",
    "    X = np.load(fd)\n",
    "    fd.close()\n",
    "\n",
    "with open('labels.npy', 'rb') as fd:\n",
    "    y = np.load(fd).astype(int)\n",
    "    fd.close()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Adjusting maximum number of epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-02    69.8%   87.5%\n",
      "  10     10 1.0e-02    69.3%   85.4%\n",
      "  10     20 1.0e-02    68.2%   85.4%\n",
      "  10     50 1.0e-02    60.4%   83.3%\n",
      "  10    100 1.0e-02    35.9%   45.8%\n",
      "  10    200 1.0e-02     8.9%   18.8%\n",
      "  10    500 1.0e-02     2.1%   16.7%\n",
      "  10   1000 1.0e-02     0.5%   14.6%\n",
      "  10   2000 1.0e-02     0.0%   12.5%\n",
      "  10   5000 1.0e-02     0.0%   10.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Adjusting the learning rate (eta)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10   5000 1.0e-04    60.4%   83.3%\n",
      "  10   5000 1.0e-03     2.1%   16.7%\n",
      "  10   5000 1.0e-02     0.0%   10.4%\n",
      "  10   5000 1.0e-01     0.0%   10.4%\n",
      "  10   5000 1.0e+00     0.0%   10.4%\n",
      "  10   5000 1.0e+01     0.0%    8.3%\n",
      "  10   5000 1.0e+02     1.0%   16.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; maxEpochs = 5000;\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Adjusting the batch size** the *bs* parameter sets every how many samples weight matrix is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "   1   5000 1.0e+01     3.1%   18.8%\n",
      "   2   5000 1.0e+01     7.8%   22.9%\n",
      "   5   5000 1.0e+01     0.0%    8.3%\n",
      "  10   5000 1.0e+01     0.0%    8.3%\n",
      "  20   5000 1.0e+01     0.0%   10.4%\n",
      "  50   5000 1.0e+01     0.0%   10.4%\n",
      " 100   5000 1.0e+01     0.0%   10.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "maxEpochs=5000; eta=1e1;\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Final classifier:** $\\;$ Training final classifier with all data available and best parameters, saving and loading to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Replace these default values with the best configuration obtained in the previous experiments\n",
    "bs=10; maxEpochs=5000; eta=1e1\n",
    "\n",
    "W = LogisticRegressionTraining(X, y, bs=bs, maxEpochs=maxEpochs)\n",
    "np.save(\"MyDigitsWeights.npy\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error of final classifier: 0.0%\n"
     ]
    }
   ],
   "source": [
    "with open('MyDigitsWeights.npy', 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "    fd.close()\n",
    "haty_test = LogisticRegressionClassification(X_test,W)\n",
    "accuracy = np.sum(haty_test==y_test)/y_test.size\n",
    "print(f\"Test error of final classifier: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to download MyDigitsWeights.npy\n",
    "# files.download('MyDigitsWeights.npy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify your own handwritten digits\n",
    "\n",
    "<p style=\"text-align: justify\">The following simple application allows you to classify your own handwritten digits. When you run this application, it shows a basic graphical interface containing a panel on which you can draw your own handwritten digits.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Before you can draw a digit, you need to click on the *pen* located on the left vertical. Then you can draw on the panel. When you need to erase what you have drawn on the panel (in order to draw a new digit), just click on *bin* located on the top menu.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">You can classify the image on the panel by clicking on the bottom bar labelled *Classify image*.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload LogisticRegression.py DigitClassifyGradioApp.py and MyDigitsWeights.npy\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from DigitClassifyGradioApp import create_interface\n",
    "\n",
    "fn = input(\"Please provide filename for weight matrix:\")\n",
    "with open(fn, 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "    fd.close()\n",
    "\n",
    "demo = create_interface(W, LogisticRegressionClassification)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
