{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3\n",
    "\n",
    "<div style=\"text-align: justify\">In this third session we will apply Logistic Regression to some classification tasks. A simple implementation of Logistic Regression and its application is provided. The final purpose of this session is to apply Logistic Regression to MyDigits dataset to train a matrix of weights that will be used in an application for Handwritten Digit Classification.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to run this code if this is the first time you are running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn scikit-learn pandas pillow gradio matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoding:** $\\;$ function to convert class labels into one-hot encoding representation\n",
    "\n",
    "$$\\operatorname{one-hot}(y)%\n",
    "=\\boldsymbol{y}%\n",
    "=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_C\\end{pmatrix}%\n",
    "=\\begin{pmatrix}\\mathbb{I}(y=1)\\\\\\vdots\\\\\\mathbb{I}(y=C)\\end{pmatrix}\\in\\{0,1\\}^C%\n",
    "\\quad\\text{with}\\quad%\n",
    "\\sum_c y_c=1$$\n",
    "\n",
    "$\\;$ where $y$ is a categorical variable that takes a value among $C$ possible categories, $\\,\\{1,\\dotsc,C\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoding(y):\n",
    "  # Unique class labels  \n",
    "  c = np.unique(y); C = c.size;\n",
    "\n",
    "  # Mapping class labels from 0 to C-1\n",
    "  # Create a mapping from values in c to their corresponding indices\n",
    "  mapping = {value: idx for idx, value in enumerate(c)}\n",
    "  # Convert y using the mapping\n",
    "  y = np.vectorize(mapping.get)(y)\n",
    "\n",
    "  # Generate one-hot encoding using np.eye\n",
    "  return np.eye(C)[y] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ComputeGradient:** $\\;$ weight update for logistic regression\n",
    "\n",
    "Gradient descent applied to logistic regression:\n",
    "\n",
    "Neg-log-likelihood (NLL): $\\;$ the NLL is a convex objective function\n",
    "$$\\operatorname{NLL}(\\mathbf{W})%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n)\n",
    "\\qquad\\text{with}\\quad%\n",
    "\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)%\n",
    "\\quad\\text{and}\\quad%\n",
    "\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$$\n",
    "\n",
    "NLL gradient:\n",
    "$$\\begin{pmatrix}%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{11}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{1C}}\\\\%\n",
    "\\vdots&\\ddots&\\vdots\\\\%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{D1}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{DC}}\\\\%\n",
    "\\end{pmatrix}\n",
    "=\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\frac{\\partial(-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n))}{\\partial \\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradient(X, Y, W):\n",
    "  N, D = X.shape\n",
    "  X = np.concatenate((np.ones((N,1)), X),axis=1)\n",
    "  N, C = Y.shape\n",
    "  Z = np.zeros((N, C)).astype(float)\n",
    "  \n",
    "  # Compute logits\n",
    "  a = X @ W\n",
    "  # Normalizing logits for robust computation to avoid overflow\n",
    "  a -= a.max(axis=1, keepdims=True) \n",
    "  # Compute softmax (probabilistic class label)\n",
    "  mu = np.exp(a)\n",
    "  mu /= np.sum(mu, axis=1, keepdims=True)\n",
    "  # Substract actual label from probabilistic class label\n",
    "  Z = mu - Y\n",
    "  # Gradient is the sum over all samples of the outer dot product\n",
    "  return (X.T @ Z)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegressionClassification:** $\\;$ classification of samples provided a weight matrix\n",
    "\n",
    "$$c(\\boldsymbol{x})%\n",
    "=\\operatorname*{argmax}\\limits_c\\;\\boldsymbol{\\mu}_c%\n",
    "\\quad\\text{with}\\quad%\n",
    "\\boldsymbol{\\mu}_c=\\mathcal{S}(\\boldsymbol{a})_c,\\quad%\n",
    "\\boldsymbol{a}=f(\\boldsymbol{x};\\mathbf{W})=\\mathbf{W}^t\\boldsymbol{x},\\quad%\n",
    "\\mathbf{W}\\in\\mathbb{R}^{D\\times C}\\quad\\text{and}\\quad\\boldsymbol{x}\\in\\mathbb{R}^D$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionClassification(X, W):\n",
    "  N, D = X.shape\n",
    "  X = np.concatenate((np.ones((N,1)), X),axis=1)\n",
    "  # Compute logits\n",
    "  a = X @ W\n",
    "  # Normalizing logits for robust computation to avoid overflow\n",
    "  a -= a.max(axis=1, keepdims=True) \n",
    "  # Compute softmax (probabilistic class label)\n",
    "  mu = np.exp(a)\n",
    "  mu /= np.sum(mu, axis=1, keepdims=True)\n",
    "  return np.argmax(mu,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegressionTraining:** $\\;$ implementation of batch training based on stochastic gradient descent\n",
    "\n",
    "Gradient descent applied to logistic regression: $\\displaystyle\\quad\\mathbf{W}_0=\\mathbf{0};\\quad\\mathbf{W}_{i+1}=\\mathbf{W}_i -\\eta_i\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}\\biggr\\vert_{\\mathbf{W}_i}\\quad i=0,1,\\ldots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionTraining(X, y, W=None, bs=1, maxEpochs=10, eta=1e-2, tol=1e-3):\n",
    "  rng = np.random.default_rng(seed=23)\n",
    "  N, D = X.shape;\n",
    "  Y = OneHotEncoding(y)\n",
    "  N, C = Y.shape\n",
    "  if W is None:  # W_0 = 0\n",
    "    W = np.zeros((1+D, C))\n",
    "  grad = np.inf; epoch = 0\n",
    "  while np.max(np.abs(grad)) > tol and epoch < maxEpochs:  # W_i+1 = W_i - eta_i * grad_W_i\n",
    "    perm = rng.permutation(N); Xperm = X[perm]; Yperm = Y[perm]\n",
    "    j = 0\n",
    "    for j in range(N//bs):\n",
    "      X_batch = Xperm[j*bs:(j+1)*bs]\n",
    "      Y_batch = Yperm[j*bs:(j+1)*bs]\n",
    "      grad = ComputeGradient(X_batch, Y_batch, W)\n",
    "      W = W - eta*grad\n",
    "    # If there are remaining samples after splitting into batches\n",
    "    if((j+1)*bs < N):\n",
    "      X_batch = Xperm[(j+1)*bs:]\n",
    "      Y_batch = Yperm[(j+1)*bs:]\n",
    "      grad = ComputeGradient(X_batch, Y_batch, W)\n",
    "      W = W - eta*grad \n",
    "    epoch += 1\n",
    "  return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and partitioning the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris(); X = iris.data.astype(np.float16); y = iris.target.astype(np.uint)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression:** $\\;$ training, classification and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = LogisticRegressionTraining(X_train,y_train, bs=10)\n",
    "haty_test = LogisticRegressionClassification(X_test,W)\n",
    "accuracy = np.sum(haty_test==y_test)/M\n",
    "print(f\"Test error: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of epochs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed the effect of *over-training* on the error rates, when the error rate in the training set still decreases while the error rate in the test set starts increasing. We must prevent over-training by applying *early stopping*, setting the maximum number of epochs to, for example, 100 epochs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate ($\\eta$/eta):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; maxEpochs = 100;\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default learning rate ($\\eta=$ 1e-2) provides good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the batch size:** the *bs* parameter sets every how many samples the weight matrix is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "maxEpochs=100; eta=1e-2;\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, a small batch size provides good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression applied to the Digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and partitioning the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = load_digits(); X = digits.images.astype(np.float16).reshape(-1, 8*8); X/=np.max(X)\n",
    "y = digits.target.astype(np.uint);\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of epochs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting learning rate (eta):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; maxEpochs = 2000;\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the batch size:** the *bs* parameter sets every how many samples the weight matrix is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "maxEpochs=2000; eta=1e-2;\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final classifier:** $\\;$ Training final classifier with all data available and best parameters, saving and loading to test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=10; maxEpochs=2000; eta=1e-2\n",
    "W = LogisticRegressionTraining(X, y, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "np.save(\"DigitsWeights.npy\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DigitsWeights.npy', 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "    fd.close()\n",
    "haty_test = LogisticRegressionClassification(X_test,W)\n",
    "accuracy = np.sum(haty_test==y_test)/y_test.size\n",
    "print(f\"Test error of final classifier: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Logistic regression applied to MyDigits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and partitioning the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload your images and labels files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('images.npy', 'rb') as fd:\n",
    "    X = np.load(fd)\n",
    "    fd.close()\n",
    "\n",
    "with open('labels.npy', 'rb') as fd:\n",
    "    y = np.load(fd).astype(int)\n",
    "    fd.close()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Adjusting maximum number of epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "# Write here the code for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Adjusting the learning rate (eta)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "# Write here the code for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Adjusting the batch size** the *bs* parameter sets every how many samples weight matrix is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "# Write here the code for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Final classifier:** $\\;$ Training final classifier with all data available and best parameters, saving and loading to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Replace these default values with the best configuration obtained in the previous experiments\n",
    "bs=10; maxEpochs=10; eta=1e-2\n",
    "\n",
    "W = LogisticRegressionTraining(X, y, bs=bs, maxEpochs=maxEpochs)\n",
    "np.save(\"MyDigitsWeights.npy\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MyDigitsWeights.npy', 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "    fd.close()\n",
    "haty_test = LogisticRegressionClassification(X_test,W)\n",
    "accuracy = np.sum(haty_test==y_test)/y_test.size\n",
    "print(f\"Test error of final classifier: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to download MyDigitsWeights.npy\n",
    "files.download('MyDigitsWeights.npy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify your own handwritten digits\n",
    "\n",
    "<p style=\"text-align: justify\">The following simple application allows you to classify your own handwritten digits. When you run this application, it shows a basic graphical interface containing a panel on which you can draw your own handwritten digits.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Before you can draw a digit, you need to click on the *pen* located on the left vertical. Then you can draw on the panel. When you need to erase what you have drawn on the panel (in order to draw a new digit), just click on *bin* located on the top menu.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">You can classify the image on the panel by clicking on the bottom bar labelled *Classify image*.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload LogisticRegression.py DigitClassifyGradioApp.py and MyDigitsWeights.npy\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from DigitClassifyGradioApp import create_interface\n",
    "\n",
    "fn = input(\"Please provide filename for weight matrix:\")\n",
    "with open(fn, 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "    fd.close()\n",
    "\n",
    "demo = create_interface(W, LogisticRegressionClassification)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
