{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Session 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will be using one of the classification tasks\n",
    "found in OpenML as the basis for a mock-up exam. More precisely, the\n",
    "task\n",
    "[*semeion*](https://www.openml.org/search?type=data&status=any&id=1501)\n",
    "is selected. This is another handwritten digit dataset. Specifically,\n",
    "it is composed by 1593 handwritten digits from around 80 different\n",
    "people. Each digit is represented as a binary image of 16x16 pixels\n",
    "(256 values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to run this code if this is the first time you are running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy>=1.24.1 (from scikit-learn)\n",
      "  Using cached numpy-2.3.5-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached numpy-2.3.5-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.2 numpy-2.3.5 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the mock-up exam, you must download the data\n",
    "(**semeion_X.npy** and **semeion_y.npy**) and the logistic regression\n",
    "library (**Logistic_Regression.py**) from poliFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "\n",
    "# You need to upload LogisticRegression.py\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# You need to upload semeion_X.npy \n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# You need to upload semeion_y.npy\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find a baseline result achieved with the logistic regression classifier using default parameters with batch size 10, and devoting 80% of the samples to training and 20% to test (with seed random_state$=$23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 12.5%\n"
     ]
    }
   ],
   "source": [
    "from LogisticRegression import LogisticRegressionClassification, LogisticRegressionTraining\n",
    "import warnings; warnings.filterwarnings(\"ignore\"); import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "X = np.load('semeion_X.npy'); y = np.load('semeion_y.npy')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)\n",
    "\n",
    "# Train and classify\n",
    "W = LogisticRegressionTraining(X_train, y_train, bs=10)\n",
    "haty_test = LogisticRegressionClassification(X_test, W)\n",
    "accuracy = np.sum(haty_test==y_test)/M\n",
    "print(f\"Test error: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Applying the logistic regression classifier with default parameter values and batch size 10, adjust the maximum number of epochs in logarithmic scale to determine an optimal value. Report the classification error rate on the training and test sets. Is overfitting observed? If so, from what epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-02    10.3%   14.4%\n",
      "  10     10 1.0e-02     8.0%   12.5%\n",
      "  10     20 1.0e-02     5.6%   10.7%\n",
      "  10     50 1.0e-02     1.8%    9.4%\n",
      "  10    100 1.0e-02     0.5%    8.5%\n",
      "  10    200 1.0e-02     0.1%    9.1%\n",
      "  10    500 1.0e-02     0.1%    9.7%\n",
      "  10   1000 1.0e-02     0.1%    9.7%\n",
      "  10   2000 1.0e-02     0.1%    9.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>\n",
    "Overfitting is observed from epoch 200 onwards, becuase the test error increases, while train error decreases.<br>\n",
    "<b>The best value is 100, as the test error is the lowest (and we want a model that generalizes).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Using maximum number of epochs 2000, learning rate (eta) 1e-2 and\n",
    "applying the logistic regression classifier, adjust the batch size in logarithmic\n",
    "scale to determine an optimal value. Report the classification error rate on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "   1   2000 1.0e-02     0.0%   10.0%\n",
      "   2   2000 1.0e-02     0.0%   10.0%\n",
      "   5   2000 1.0e-02     0.0%    9.7%\n",
      "  10   2000 1.0e-02     0.1%    9.7%\n",
      "  20   2000 1.0e-02     0.0%    9.7%\n",
      "  50   2000 1.0e-02     0.0%    9.7%\n",
      " 100   2000 1.0e-02     0.1%    9.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "maxEpochs=2000; eta=1e-2;\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>\n",
    "With a batch size of 5, we get the lower training error and test error (0% and 9,7% respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Applying the logistic regression classifier with default parameter values and\n",
    "batch size 10, adjust both the maximum number of epochs and the learning rate\n",
    "(eta) to determine the optimal values. Use in both cases a logarithmic\n",
    "scale. Report the classification error rate on the training and test\n",
    "sets. Discuss the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-04    38.5%   42.9%\n",
      "  10      5 1.0e-03    21.7%   25.7%\n",
      "  10      5 1.0e-02    10.3%   14.4%\n",
      "  10      5 1.0e-01     2.2%   11.0%\n",
      "  10      5 1.0e+00     0.2%   11.0%\n",
      "  10      5 1.0e+01     6.7%   13.5%\n",
      "  10      5 1.0e+02     8.0%   11.6%\n",
      "  10     10 1.0e-04    35.8%   42.6%\n",
      "  10     10 1.0e-03    17.0%   19.7%\n",
      "  10     10 1.0e-02     8.0%   12.5%\n",
      "  10     10 1.0e-01     0.5%    9.7%\n",
      "  10     10 1.0e+00     0.2%   11.0%\n",
      "  10     10 1.0e+01     6.7%   13.5%\n",
      "  10     10 1.0e+02     8.0%   11.6%\n",
      "  10     20 1.0e-04    30.5%   34.8%\n",
      "  10     20 1.0e-03    14.1%   17.9%\n",
      "  10     20 1.0e-02     5.6%   10.7%\n",
      "  10     20 1.0e-01     0.1%   10.0%\n",
      "  10     20 1.0e+00     0.2%   11.0%\n",
      "  10     20 1.0e+01     6.7%   13.5%\n",
      "  10     20 1.0e+02     8.0%   11.6%\n",
      "  10     50 1.0e-04    21.5%   25.7%\n",
      "  10     50 1.0e-03    10.7%   14.4%\n",
      "  10     50 1.0e-02     1.8%    9.4%\n",
      "  10     50 1.0e-01     0.0%    9.7%\n",
      "  10     50 1.0e+00     0.2%   11.0%\n",
      "  10     50 1.0e+01     6.7%   13.5%\n",
      "  10     50 1.0e+02     8.0%   11.6%\n",
      "  10    100 1.0e-04    17.2%   20.7%\n",
      "  10    100 1.0e-03     8.0%   11.6%\n",
      "  10    100 1.0e-02     0.5%    8.5%\n",
      "  10    100 1.0e-01     0.0%    9.7%\n",
      "  10    100 1.0e+00     0.2%   11.0%\n",
      "  10    100 1.0e+01     6.7%   13.5%\n",
      "  10    100 1.0e+02     8.0%   11.6%\n",
      "  10    200 1.0e-04    14.1%   17.6%\n",
      "  10    200 1.0e-03     5.7%   10.7%\n",
      "  10    200 1.0e-02     0.1%    9.1%\n",
      "  10    200 1.0e-01     0.0%    9.7%\n",
      "  10    200 1.0e+00     0.2%   11.0%\n",
      "  10    200 1.0e+01     6.7%   13.5%\n",
      "  10    200 1.0e+02     8.0%   11.6%\n",
      "  10    500 1.0e-04    10.7%   14.4%\n",
      "  10    500 1.0e-03     1.7%    9.4%\n",
      "  10    500 1.0e-02     0.1%    9.7%\n",
      "  10    500 1.0e-01     0.0%    9.7%\n",
      "  10    500 1.0e+00     0.2%   11.0%\n",
      "  10    500 1.0e+01     6.7%   13.5%\n",
      "  10    500 1.0e+02     8.0%   11.6%\n",
      "  10   1000 1.0e-04     8.0%   11.6%\n",
      "  10   1000 1.0e-03     0.5%    9.4%\n",
      "  10   1000 1.0e-02     0.1%    9.7%\n",
      "  10   1000 1.0e-01     0.0%    9.7%\n",
      "  10   1000 1.0e+00     0.2%   11.0%\n",
      "  10   1000 1.0e+01     6.7%   13.5%\n",
      "  10   1000 1.0e+02     8.0%   11.6%\n",
      "  10   2000 1.0e-04     5.7%   10.7%\n",
      "  10   2000 1.0e-03     0.2%    9.7%\n",
      "  10   2000 1.0e-02     0.1%    9.7%\n",
      "  10   2000 1.0e-01     0.0%    9.7%\n",
      "  10   2000 1.0e+00     0.2%   11.0%\n",
      "  10   2000 1.0e+01     6.7%   13.5%\n",
      "  10   2000 1.0e+02     8.0%   11.6%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "    for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "        W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "        haty_train = LogisticRegressionClassification(X_train,W)\n",
    "        acc_train = np.sum(haty_train==y_train)/N\n",
    "        haty_test = LogisticRegressionClassification(X_test,W)\n",
    "        acc_test = np.sum(haty_test==y_test)/M\n",
    "        print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>\n",
    "We can use 100 epochs and 1.0e-02 as learning rate, which provides the smallest test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "According to the results you have obtained, could you claim that this task is linearly separable? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
