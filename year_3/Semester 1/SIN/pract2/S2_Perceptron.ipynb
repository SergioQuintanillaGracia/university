{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "In this second session we will apply the Perceptron algorithm to some classification tasks. A simple implementation of the Perceptron algorithm and its application is provided. The final purpose of this session is to apply the Perceptron algorithm to MyDigits dataset to train a matrix of weights that will be used in an application for Handwritten Digit Classification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to run this code if this is the first time you are running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in ./.venv/lib64/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib64/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib64/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: pillow in ./.venv/lib64/python3.13/site-packages (11.3.0)\n",
      "Requirement already satisfied: gradio in ./.venv/lib64/python3.13/site-packages (5.49.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib64/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib64/python3.13/site-packages (from seaborn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib64/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.121.2)\n",
      "Requirement already satisfied: ffmpy in ./.venv/lib64/python3.13/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==1.13.3 in ./.venv/lib64/python3.13/site-packages (from gradio) (1.13.3)\n",
      "Requirement already satisfied: groovy~=0.1 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in ./.venv/lib64/python3.13/site-packages (from gradio) (1.1.4)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: orjson~=3.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: packaging in ./.venv/lib64/python3.13/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.11.10)\n",
      "Requirement already satisfied: pydub in ./.venv/lib64/python3.13/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: ruff>=0.9.3 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.14.5)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.49.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib64/python3.13/site-packages (from gradio-client==1.13.3->gradio) (2025.10.0)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in ./.venv/lib64/python3.13/site-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in ./.venv/lib64/python3.13/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib64/python3.13/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib64/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib64/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn scikit-learn pandas pillow gradio matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron Classification:** classification of samples provided a weight matrix. Samples need to be prefixed by 1\n",
    "\n",
    "$$c(\\boldsymbol{x}) = \\operatorname*{argmax}_{c} g_{c}(\\boldsymbol{x})\\text{, with }g_{c}(\\boldsymbol{x})=\\boldsymbol{w}_{c}^{t}\\boldsymbol{x}\\text{ for all }c$$\n",
    "\n",
    "where $\\boldsymbol{x} = (1, x_1, ..., x_D)^t$, $\\mathbf{W} = (\\boldsymbol{w}_1, \\boldsymbol{w}_2, ...,  \\boldsymbol{w}_C)$ and $\\boldsymbol{w}_c = (w_{c0}, w_{c1}, ..., w_{cD})^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptronClassification(X, W):\n",
    "  Xh = np.hstack([np.ones((len(X), 1)), X])\n",
    "  return np.argmax(Xh @ W, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PerceptronTraining:** $\\;$ Perceptron learns a matrix of weights $\\mathbf{W}^*$ that minimizes the number of training errors (with margin $b$)\n",
    "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{ I}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{ x}_n\\biggr)$$\n",
    "\n",
    "It returns weights in homogeneous notation, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$  together with the number of errors and iterations executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Input:** $\\;$ data $\\;\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\}\\quad$ weights $\\;\\mathbf{W}=\\{\\boldsymbol{w}_c\\}\\quad$ learning rate $\\;\\alpha\\in\\mathbb{R}^{>0}\\quad$ margin $\\;b\\in\\mathbb{R}^{\\geq 0}$ <br>\n",
    "> **Output:** $\\;$ optimized weights $\\;\\mathbf{W}^*=\\{\\boldsymbol{w}_c\\}^*$ <br>\n",
    "> `repeat` <br>\n",
    ">> `for all` $\\;$ training sample $\\,\\boldsymbol{x}_n$ <br>\n",
    ">>> *err* = `False` <br>\n",
    ">>> `for all` $\\;$ class $\\,c\\neq y_n$ <br>\n",
    ">>>> `if` $\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b>\\boldsymbol{w}_{y_n}^t\\boldsymbol{x}_n:\\quad\\boldsymbol{w}_c=\\boldsymbol{w}_c-\\alpha\\boldsymbol{x}_n;\\quad$ *err* = `True` <br>\n",
    ">>>\n",
    ">>> `if` $\\;$ *err*: $\\quad \\boldsymbol{w}_{y_n}=\\boldsymbol{w}_{y_n}+\\alpha\\boldsymbol{x}_n$\n",
    ">\n",
    "> `until` $\\;$ no training sample is misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptronTraining(X, y, b=0.1, a=1.0, K=200):\n",
    "    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n",
    "    for k in range(1, K+1):  # for K iterations\n",
    "        E = 0\n",
    "        for n in range(N):  # for every training sample\n",
    "            xn = np.array([1, *X[n, :]])\n",
    "            cn = np.squeeze(np.where(Y==y[n]))  # Mapping to class labels from 0 to C-1 (for algorithmic simplicity)\n",
    "            gn = W[:,cn].T @ xn; err = False\n",
    "            for c in np.arange(C):  # for every class \n",
    "                if c != cn and W[:,c].T @ xn + b >= gn:\n",
    "                    W[:, c] = W[:, c] - a*xn; err = True\n",
    "            if err:\n",
    "                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n",
    "        if E == 0:\n",
    "            break\n",
    "    return W, E, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ we also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150, 1) \n",
      " [[5.1015625  3.5        1.40039062 0.19995117 0.        ]\n",
      " [4.8984375  3.         1.40039062 0.19995117 0.        ]\n",
      " [4.69921875 3.19921875 1.29980469 0.19995117 0.        ]\n",
      " [4.6015625  3.09960938 1.5        0.19995117 0.        ]\n",
      " [5.         3.59960938 1.40039062 0.19995117 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(); X = iris.data.astype(np.float16)\n",
    "y = iris.target.astype(np.uint).reshape(-1, 1)\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ We create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning a (linear) classifier with Perceptron:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations executed:  200\n",
      "Number of training errors:  2\n",
      "Weight vectors of the classes (in columns and with homogeneous notation):\n",
      " [[  10.           85.         -142.        ]\n",
      " [ -49.421875    -68.19140625 -176.47265625]\n",
      " [  50.171875     -1.72460938 -181.06445312]\n",
      " [-189.91210938  -87.70507812   68.69726562]\n",
      " [ -86.40258789 -137.78149414  157.88415527]]\n"
     ]
    }
   ],
   "source": [
    "W, E, k = PerceptronTraining(X_train, y_train)\n",
    "print(\"Number of iterations executed: \", k)\n",
    "print(\"Number of training errors: \", E)\n",
    "print(\"Weight vectors of the classes (in columns and with homogeneous notation):\\n\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of test error rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on test: 16.7%\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = PerceptronClassification(X_test,W)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Error rate on test: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of iterations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 1.0e-01      1  49.2%  33.3%\n",
      "1.0e+00 1.0e-01      2  31.7%  50.0%\n",
      "1.0e+00 1.0e-01      5  14.2%  73.3%\n",
      "1.0e+00 1.0e-01     10  12.5%  56.7%\n",
      "1.0e+00 1.0e-01     20  14.2%  26.7%\n",
      "1.0e+00 1.0e-01     50   8.3%  16.7%\n",
      "1.0e+00 1.0e-01    100   9.2%  26.7%\n",
      "1.0e+00 1.0e-01    200   1.7%  16.7%\n",
      "1.0e+00 1.0e-01    500   2.5%   3.3%\n",
      "1.0e+00 1.0e-01   1000   2.5%  13.3%\n",
      "1.0e+00 1.0e-01   2000   5.0%   3.3%\n",
      "1.0e+00 1.0e-01   5000   1.7%   6.7%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; a = 1.0\n",
    "for K in (1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate (alpha):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e-03 1.0e-01    500   8.3%   3.3%\n",
      "1.0e-02 1.0e-01    500   2.5%   3.3%\n",
      "1.0e-01 1.0e-01    500   4.2%  16.7%\n",
      "1.0e+00 1.0e-01    500   2.5%   3.3%\n",
      "1.0e+01 1.0e-01    500   4.2%  16.7%\n",
      "1.0e+02 1.0e-01    500   4.2%  16.7%\n",
      "1.0e+03 1.0e-01    500   0.8%  16.7%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; K = 500\n",
    "for a in (1e-3, 1e-2, 1e-1, 1e-0, 1e1, 1e2, 1e3):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the margin (b):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 0.0e+00    500   0.8%  16.7%\n",
      "1.0e+00 1.0e-02    500   4.2%  16.7%\n",
      "1.0e+00 1.0e-01    500   2.5%   3.3%\n",
      "1.0e+00 1.0e+00    500   4.2%  16.7%\n",
      "1.0e+00 1.0e+01    500   2.5%   3.3%\n",
      "1.0e+00 1.0e+02    500   8.3%   3.3%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "a = 1.0; K = 500\n",
    "for b in (.0, .01, .1, 1, 10, 100):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results:** $\\;$ the training data does not appear to be linearly separable; it is not clear that a margin greater than zero can improve results, especially since we only have $30$ test samples; with a margin $b=0.1$ we have already seen that an error (in test) of $3.3\\%$ is obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ we also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797, 1) \n",
      " [[0.     0.     0.3125 0.8125 0.5625 0.0625 0.     0.     0.     0.\n",
      "  0.8125 0.9375 0.625  0.9375 0.3125 0.     0.     0.1875 0.9375 0.125\n",
      "  0.     0.6875 0.5    0.     0.     0.25   0.75   0.     0.     0.5\n",
      "  0.5    0.     0.     0.3125 0.5    0.     0.     0.5625 0.5    0.\n",
      "  0.     0.25   0.6875 0.     0.0625 0.75   0.4375 0.     0.     0.125\n",
      "  0.875  0.3125 0.625  0.75   0.     0.     0.     0.     0.375  0.8125\n",
      "  0.625  0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.75   0.8125 0.3125 0.     0.     0.     0.\n",
      "  0.     0.6875 1.     0.5625 0.     0.     0.     0.     0.1875 0.9375\n",
      "  1.     0.375  0.     0.     0.     0.4375 0.9375 1.     1.     0.125\n",
      "  0.     0.     0.     0.     0.0625 1.     1.     0.1875 0.     0.\n",
      "  0.     0.     0.0625 1.     1.     0.375  0.     0.     0.     0.\n",
      "  0.0625 1.     1.     0.375  0.     0.     0.     0.     0.     0.6875\n",
      "  1.     0.625  0.     0.     1.    ]\n",
      " [0.     0.     0.     0.25   0.9375 0.75   0.     0.     0.     0.\n",
      "  0.1875 1.     0.9375 0.875  0.     0.     0.     0.     0.5    0.8125\n",
      "  0.5    1.     0.     0.     0.     0.     0.0625 0.375  0.9375 0.6875\n",
      "  0.     0.     0.     0.0625 0.5    0.8125 0.9375 0.0625 0.     0.\n",
      "  0.     0.5625 1.     1.     0.3125 0.     0.     0.     0.     0.1875\n",
      "  0.8125 1.     1.     0.6875 0.3125 0.     0.     0.     0.     0.1875\n",
      "  0.6875 1.     0.5625 0.     2.    ]\n",
      " [0.     0.     0.4375 0.9375 0.8125 0.0625 0.     0.     0.     0.5\n",
      "  0.8125 0.375  0.9375 0.25   0.     0.     0.     0.125  0.0625 0.8125\n",
      "  0.8125 0.     0.     0.     0.     0.     0.125  0.9375 0.6875 0.0625\n",
      "  0.     0.     0.     0.     0.     0.0625 0.75   0.75   0.0625 0.\n",
      "  0.     0.     0.     0.     0.0625 0.625  0.5    0.     0.     0.\n",
      "  0.5    0.25   0.3125 0.875  0.5625 0.     0.     0.     0.4375 0.8125\n",
      "  0.8125 0.5625 0.     0.     3.    ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits(); X = digits.images.astype(np.float16).reshape(-1, 8*8); X/=np.max(X)\n",
    "y = digits.target.astype(np.uint).reshape(-1, 1)\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:4, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ We create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (360, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of iterations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 1.0e-01      1  25.1%  14.7%\n",
      "1.0e+00 1.0e-01      2  13.2%   8.1%\n",
      "1.0e+00 1.0e-01      5   8.4%   8.1%\n",
      "1.0e+00 1.0e-01     10   5.6%   7.5%\n",
      "1.0e+00 1.0e-01     20   2.9%   6.7%\n",
      "1.0e+00 1.0e-01     50   1.9%   5.8%\n",
      "1.0e+00 1.0e-01    100   0.8%   4.7%\n",
      "1.0e+00 1.0e-01    111   0.0%   4.4%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; a = 1.0\n",
    "for K in (1, 2, 5, 10, 20, 50, 100, 200):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate (alpha):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e-03 1.0e-01    200   5.5%   3.6%\n",
      "1.0e-02 1.0e-01    200   0.2%   5.3%\n",
      "1.0e-01 1.0e-01    113   0.0%   5.3%\n",
      "1.0e+00 1.0e-01    111   0.0%   4.4%\n",
      "1.0e+01 1.0e-01    130   0.0%   3.6%\n",
      "1.0e+02 1.0e-01    112   0.0%   4.2%\n",
      "1.0e+03 1.0e-01    112   0.0%   4.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; K = 200\n",
    "for a in (1e-3, 1e-2, 1e-1, 1e-0, 1e1, 1e2, 1e3):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the margin (b):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+01 0.0e+00    112   0.0%   4.2%\n",
      "1.0e+01 1.0e-02    112   0.0%   4.2%\n",
      "1.0e+01 1.0e-01    130   0.0%   3.6%\n",
      "1.0e+01 1.0e+00    111   0.0%   4.4%\n",
      "1.0e+01 1.0e+01    113   0.0%   5.3%\n",
      "1.0e+01 1.0e+02    187   0.0%   5.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "a = 1e1; K = 200\n",
    "for b in (.0, .01, .1, 1, 10, 100):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results:** $\\;$ the training data is linearly separable with training error equal to zero. In this case, it seems that small margins provide similar results on the test set with a lowest value of $3.6\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to MyDigits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload your images and labels files\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('images.npy', 'rb') as fd:\n",
    "    X = np.load(fd)\n",
    "\n",
    "with open('labels.npy', 'rb') as fd:\n",
    "    y = np.load(fd).astype(int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 64) (48, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of iterations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 1.0e-01      1  46.9%  27.1%\n",
      "1.0e+00 1.0e-01      2  15.6%  18.8%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; a = 1.0\n",
    "for K in (1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate (alpha):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e-03 1.0e-01    119   0.0%  10.4%\n",
      "1.0e-02 1.0e-01     20   0.0%   8.3%\n",
      "1.0e-01 1.0e-01      9   0.0%   8.3%\n",
      "1.0e+00 1.0e-01      5   0.0%  10.4%\n",
      "1.0e+01 1.0e-01      6   0.0%  16.7%\n",
      "1.0e+02 1.0e-01      6   0.0%  10.4%\n",
      "1.0e+03 1.0e-01      6   0.0%  10.4%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; K = 200\n",
    "for a in (1e-3, 1e-2, 1e-1, 1e-0, 1e1, 1e2, 1e3):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the margin (b):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+01 0.0e+00      6   0.0%  10.4%\n",
      "1.0e+01 1.0e-02      6   0.0%  10.4%\n",
      "1.0e+01 1.0e-01      6   0.0%  16.7%\n",
      "1.0e+01 1.0e+00      5   0.0%  10.4%\n",
      "1.0e+01 1.0e+01      9   0.0%   8.3%\n",
      "1.0e+01 1.0e+02     20   0.0%   8.3%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "a = 1e1; K = 200\n",
    "for b in (.0, .01, .1, 1, 10, 100):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final classifier:** $\\;$ Training final classifier with best parameters, saving and loading to test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.1; a = 1.0; K = 200 # Replace with the best configuration obtained in the previous experiments\n",
    "W, E, k = PerceptronTraining(X, y, b=b, a=a, K=K)\n",
    "np.save(\"MyDigitsWeights.npy\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error of final classifier: 0.0%\n"
     ]
    }
   ],
   "source": [
    "with open('MyDigitsWeights.npy', 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "y_test_pred = PerceptronClassification(X_test,W)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Test error of final classifier: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to download MyDigitsWeights.npy\n",
    "# files.download('MyDigitsWeights.npy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify your own handwritten digits\n",
    "\n",
    "<p style=\"text-align: justify\">The following simple application allows you to classify your own handwritten digits. When you run this application, it shows a basic graphical interface containing a panel on which you can draw your own handwritten digits.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Before you can draw a digit, you need to click on the *pen* locate on the left vertical. Then you can draw on the panel. If you need to erase what you have drawn on the panel, just click on *bin* located on the top menu.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">You can classify the image on the panel by clicking on the bottom bar labeled with *Classify image\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload DigitClassifyGradioApp.py\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DigitClassifyGradioApp import create_interface\n",
    "\n",
    "fn = input(\"Please provide filename for weight matrix:\")\n",
    "with open(fn, 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "\n",
    "demo = create_interface(W, PerceptronClassification)\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
